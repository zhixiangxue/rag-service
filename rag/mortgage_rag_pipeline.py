#!/usr/bin/env python3
"""
Mortgage Guidelines RAG Pipeline

å¤„ç†æˆ¿è´·æŒ‡å—æ–‡æ¡£çš„å®Œæ•´ RAG æµç¨‹ï¼š
1. æ–‡æ¡£è¯»å– (PDF)
2. æ–‡æ¡£åˆ‡åˆ† (åŸºäº Markdown æ ‡é¢˜ + é€’å½’åˆå¹¶)
3. è¡¨æ ¼å¤„ç† (è§£æ + æ‘˜è¦)
4. å…ƒæ•°æ®æå– (å…³é”®è¯)
5. ç´¢å¼•æ„å»º (å‘é‡ + å…¨æ–‡)
6. æ£€ç´¢æµ‹è¯• (èåˆæ£€ç´¢)
7. åå¤„ç† (è¿‡æ»¤ + å»é‡ + ä¸Šä¸‹æ–‡å¢å¼º)

è¿è¡Œå‰å‡†å¤‡ï¼š
1. å¯åŠ¨ Meilisearch: ./meilisearch
2. è®¾ç½®ç¯å¢ƒå˜é‡ .env:
   BAILIAN_API_KEY=your-api-key
3. PDF æ–‡ä»¶åœ¨ rag/files/ ç›®å½•ä¸‹
"""

import os
import sys
import time
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from zag.readers.docling import DoclingReader
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
from zag.splitters import MarkdownHeaderSplitter, RecursiveMergingSplitter
from zag.extractors import TableExtractor, KeywordExtractor
from zag.embedders import Embedder
from zag.storages.vector import ChromaVectorStore
from zag.indexers import VectorIndexer, FullTextIndexer
from zag.retrievers import VectorRetriever, FullTextRetriever, QueryFusionRetriever, FusionMode
from zag.postprocessors import (
    SimilarityFilter,
    Deduplicator,
    ContextAugmentor,
    ChainPostprocessor,
)
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.table import Table
from rich.panel import Panel

console = Console()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®å‚æ•°
API_KEY = os.getenv("BAILIAN_API_KEY")
EMBEDDING_MODEL = "text-embedding-v3"
LLM_MODEL = "qwen-plus"
EMBEDDING_URI = f"bailian/{EMBEDDING_MODEL}"
LLM_URI = f"bailian/{LLM_MODEL}"
MEILISEARCH_URL = "http://127.0.0.1:7700"
FILES_DIR = Path(__file__).parent / "files"
OUTPUT_DIR = Path(__file__).parent / "output"
CHROMA_PERSIST_DIR = OUTPUT_DIR / "chroma_db"

# æµç¨‹æ§åˆ¶é…ç½®
RUN_UNTIL_STEP = 1  # è¿è¡Œåˆ°ç¬¬å‡ æ­¥å°±åœæ­¢ (1-7)ï¼Œè®¾ç½®ä¸º 7 è¡¨ç¤ºè¿è¡Œå®Œæ•´æµç¨‹

# åˆ›å»ºè¾“å‡ºç›®å½•
OUTPUT_DIR.mkdir(exist_ok=True)


def print_section(title: str, char: str = "="):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    console.print(f"\n{char * 70}")
    console.print(f"  {title}", style="bold cyan")
    console.print(f"{char * 70}\n")


# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨å¼€å§‹æ—¶é—´
_pipeline_start_time = None


def should_stop_after_step(step_num: int):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨å½“å‰æ­¥éª¤ååœæ­¢"""
    if step_num >= RUN_UNTIL_STEP:
        total_time = time.time() - _pipeline_start_time
        console.print(f"\nâœ… æ‰§è¡Œåˆ° Step {step_num}ï¼Œå®Œæˆï¼(è€—æ—¶ {total_time:.2f}s)", style="bold green")
        console.print(f"\nğŸ’¡ æç¤º: ä¿®æ”¹ RUN_UNTIL_STEP é…ç½®å¯ä»¥è¿è¡Œæ›´å¤šæ­¥éª¤", style="yellow")
        sys.exit(0)


def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    print_section("ğŸ” æ£€æŸ¥å‰ç½®æ¡ä»¶")
    
    issues = []
    
    # æ£€æŸ¥ API Key
    if not API_KEY:
        issues.append("âŒ .env æ–‡ä»¶ä¸­æœªæ‰¾åˆ° BAILIAN_API_KEY")
    else:
        console.print(f"âœ… API Key å·²æ‰¾åˆ°: {API_KEY[:10]}...")
    
    # æ£€æŸ¥ PDF æ–‡ä»¶
    pdf_files = list(FILES_DIR.glob("*.pdf"))
    if not pdf_files:
        issues.append(f"âŒ æœªæ‰¾åˆ° PDF æ–‡ä»¶: {FILES_DIR}")
    else:
        console.print(f"âœ… æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶:")
        for pdf in pdf_files:
            size_mb = pdf.stat().st_size / (1024 * 1024)
            console.print(f"   - {pdf.name} ({size_mb:.1f} MB)")
    
    # æ£€æŸ¥ Meilisearch
    try:
        import meilisearch
        client = meilisearch.Client(MEILISEARCH_URL)
        health = client.health()
        if health.get("status") == "available":
            console.print(f"âœ… Meilisearch è¿è¡Œä¸­: {MEILISEARCH_URL}")
        else:
            issues.append("âŒ Meilisearch ä¸å¯ç”¨")
    except Exception as e:
        issues.append(f"âŒ æ— æ³•è¿æ¥åˆ° Meilisearch: {e}")
    
    if issues:
        console.print("\nâš ï¸  å‘ç°é—®é¢˜:", style="bold yellow")
        for issue in issues:
            console.print(f"  {issue}")
        return False
    
    console.print("\nâœ… æ‰€æœ‰å‰ç½®æ¡ä»¶æ»¡è¶³!", style="bold green")
    return True


async def step1_read_documents():
    """æ­¥éª¤ 1: è¯»å–æ‰€æœ‰ PDF æ–‡æ¡£"""
    print_section("ğŸ“„ æ­¥éª¤ 1: è¯»å–æ–‡æ¡£", "-")
    
    pdf_files = sorted(FILES_DIR.glob("*.pdf"))
    console.print(f"å‡†å¤‡è¯»å– {len(pdf_files)} ä¸ª PDF æ–‡ä»¶...")
    
    # é…ç½® DoclingReader
    pdf_options = PdfPipelineOptions()
    pdf_options.accelerator_options = AcceleratorOptions(
        num_threads=8,
        device=AcceleratorDevice.CPU
    )
    
    reader = DoclingReader(pdf_pipeline_options=pdf_options)
    documents = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        console=console
    ) as progress:
        task = progress.add_task("[cyan]è¯»å–æ–‡æ¡£...", total=len(pdf_files))
        
        for pdf_path in pdf_files:
            console.print(f"\næ­£åœ¨è¯»å–: {pdf_path.name}")
            doc = reader.read(str(pdf_path))
            documents.append(doc)
            
            console.print(f"  âœ… å†…å®¹é•¿åº¦: {len(doc.content):,} å­—ç¬¦")
            console.print(f"  âœ… é¡µæ•°: {len(doc.pages)}")
            if doc.metadata.custom:
                console.print(f"  âœ… æ–‡æœ¬é¡¹: {doc.metadata.custom.get('text_items_count', 0)}")
                console.print(f"  âœ… è¡¨æ ¼é¡¹: {doc.metadata.custom.get('table_items_count', 0)}")
            
            # ä¿å­˜ Markdown å†…å®¹
            markdown_path = OUTPUT_DIR / f"{pdf_path.stem}_content.md"
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(doc.content)
            console.print(f"  âœ… Markdown å·²ä¿å­˜: {markdown_path.name}")
            
            progress.update(task, advance=1)
    
    console.print(f"\nâœ… å…±è¯»å– {len(documents)} ä¸ªæ–‡æ¡£", style="bold green")
    return documents


async def step2_split_documents(documents):
    """æ­¥éª¤ 2: åˆ‡åˆ†æ‰€æœ‰æ–‡æ¡£"""
    print_section("ğŸ”ª æ­¥éª¤ 2: åˆ‡åˆ†æ–‡æ¡£", "-")
    
    console.print("ä½¿ç”¨ RecursiveMergingSplitter (ç›®æ ‡: 800 tokens)...")
    base_splitter = MarkdownHeaderSplitter()
    merger = RecursiveMergingSplitter(
        base_splitter=base_splitter,
        target_token_size=800
    )
    
    all_units = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        console=console
    ) as progress:
        task = progress.add_task("[cyan]åˆ‡åˆ†æ–‡æ¡£...", total=len(documents))
        
        for doc in documents:
            units = doc.split(merger)
            all_units.extend(units)
            console.print(f"  {doc.metadata.filename}: {len(units)} ä¸ªå•å…ƒ")
            progress.update(task, advance=1)
    
    # è®¡ç®— token ç»Ÿè®¡
    import tiktoken
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = [len(tokenizer.encode(u.content)) for u in all_units]
    
    console.print(f"\nâœ… åˆ‡åˆ†å®Œæˆ:", style="bold green")
    console.print(f"   - æ€»å•å…ƒæ•°: {len(all_units)}")
    console.print(f"   - Token èŒƒå›´: {min(tokens)}-{max(tokens)} (å¹³å‡: {sum(tokens)//len(tokens)})")
    
    return all_units


async def step3_process_tables(units):
    """æ­¥éª¤ 3: å¤„ç†è¡¨æ ¼ (è§£æ + æ‘˜è¦)"""
    print_section("ğŸ“Š æ­¥éª¤ 3: å¤„ç†è¡¨æ ¼", "-")
    
    console.print("ä½¿ç”¨ LLM æå–è¡¨æ ¼ä¿¡æ¯...")
    extractor = TableExtractor(
        llm_uri=LLM_URI,
        api_key=API_KEY
    )
    
    # æ‰¹é‡æå–
    results = await extractor.aextract(units)
    
    # æ›´æ–° embedding_content
    for unit, metadata in zip(units, results):
        if metadata.get("embedding_content"):
            unit.embedding_content = metadata["embedding_content"]
    
    console.print(f"âœ… å·²å¤„ç† {len(units)} ä¸ªå•å…ƒ", style="bold green")
    return units


async def step4_extract_metadata(units):
    """æ­¥éª¤ 4: æå–å…ƒæ•°æ® (å…³é”®è¯)"""
    print_section("ğŸ·ï¸  æ­¥éª¤ 4: æå–å…ƒæ•°æ®", "-")
    
    console.print("ä¸ºæ‰€æœ‰å•å…ƒæå–å…³é”®è¯...")
    extractor = KeywordExtractor(
        llm_uri=LLM_URI,
        api_key=API_KEY,
        num_keywords=5
    )
    
    # æ‰¹é‡æå–
    results = await extractor.aextract(units)
    
    # æ›´æ–°å…ƒæ•°æ®
    for unit, metadata in zip(units, results):
        unit.metadata.custom.update(metadata)
    
    console.print(f"âœ… å·²ä¸º {len(units)} ä¸ªå•å…ƒæå–å…³é”®è¯", style="bold green")
    console.print("\nç¤ºä¾‹å…³é”®è¯ (å‰ 3 ä¸ªå•å…ƒ):")
    for i, unit in enumerate(units[:3], 1):
        keywords = unit.metadata.custom.get("excerpt_keywords", [])
        console.print(f"   {i}. {keywords}")
    
    return units


async def step5_build_indices(units):
    """æ­¥éª¤ 5: æ„å»ºç´¢å¼• (å‘é‡ + å…¨æ–‡)"""
    print_section("ğŸ“š æ­¥éª¤ 5: æ„å»ºç´¢å¼•", "-")
    
    # ä¿å­˜ units åˆ° JSON ä»¥ä¾›æ£€æŸ¥
    import json
    units_json_path = OUTPUT_DIR / "units_data.json"
    units_data = [unit.model_dump(mode='json') for unit in units]
    
    with open(units_json_path, 'w', encoding='utf-8') as f:
        json.dump(units_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"Units æ•°æ®å·²ä¿å­˜åˆ°: {units_json_path}")
    console.print(f"æ€»å•å…ƒæ•°: {len(units)}\n")
    
    # 5.1 å‘é‡ç´¢å¼•
    console.print("æ„å»ºå‘é‡ç´¢å¼•...")
    embedder = Embedder(
        EMBEDDING_URI,
        api_key=API_KEY
    )
    
    vector_store = ChromaVectorStore.local(
        path=str(CHROMA_PERSIST_DIR),
        collection_name="mortgage_guidelines",
        embedder=embedder
    )
    console.print(f"   æŒä¹…åŒ–ç›®å½•: {CHROMA_PERSIST_DIR}")
    
    vector_indexer = VectorIndexer(vector_store=vector_store)
    # æ¸…ç©ºç°æœ‰æ•°æ®
    await vector_indexer.aclear()
    await vector_indexer.aadd(units)
    console.print(f"   âœ… å‘é‡ç´¢å¼•å·²æ„å»º: {vector_indexer.count()} ä¸ªå•å…ƒ", style="bold green")
    
    # 5.2 å…¨æ–‡ç´¢å¼•
    console.print("\næ„å»ºå…¨æ–‡ç´¢å¼•...")
    fulltext_indexer = FullTextIndexer(
        url=MEILISEARCH_URL,
        index_name="mortgage_guidelines",
        primary_key="unit_id"
    )
    
    # æ¸…ç©ºç°æœ‰æ•°æ®
    fulltext_indexer.clear()
    fulltext_indexer.configure_settings(
        searchable_attributes=["content", "context_path"],
        filterable_attributes=["unit_type", "source_doc_id"],
        sortable_attributes=["created_at"],
    )
    fulltext_indexer.add(units)
    console.print(f"   âœ… å…¨æ–‡ç´¢å¼•å·²æ„å»º: {fulltext_indexer.count()} ä¸ªå•å…ƒ", style="bold green")
    
    return vector_indexer, fulltext_indexer


async def step6_test_retrieval(vector_indexer, fulltext_indexer):
    """æ­¥éª¤ 6: æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    print_section("ğŸ” æ­¥éª¤ 6: æµ‹è¯•æ£€ç´¢", "-")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "FHA è´·æ¬¾çš„é¦–ä»˜è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
        "VA è´·æ¬¾çš„èµ„æ ¼æ¡ä»¶æœ‰å“ªäº›ï¼Ÿ",
        "Fannie Mae çš„ LTV è¦æ±‚",
        "USDA è´·æ¬¾é€‚ç”¨çš„åœ°åŒº",
        "Freddie Mac çš„åˆ©ç‡æ”¿ç­–",
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    vector_retriever = VectorRetriever(vector_store=vector_indexer.vector_store, top_k=5)
    fulltext_retriever = FullTextRetriever(url=MEILISEARCH_URL, index_name="mortgage_guidelines", top_k=5)
    
    # åˆ›å»ºèåˆæ£€ç´¢å™¨
    fusion_retriever = QueryFusionRetriever(
        retrievers=[vector_retriever, fulltext_retriever],
        mode=FusionMode.RECIPROCAL_RANK,
        top_k=3
    )
    
    console.print("æµ‹è¯•æŸ¥è¯¢ç¤ºä¾‹:\n")
    for i, query in enumerate(test_queries[:3], 1):
        console.print(f"[bold cyan]{i}. æŸ¥è¯¢:[/bold cyan] {query}")
        
        start = time.time()
        results = fusion_retriever.retrieve(query)
        elapsed = time.time() - start
        
        console.print(f"   âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ ({elapsed*1000:.0f}ms)")
        
        if results:
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç»“æœ
            top_result = results[0]
            preview = top_result.content[:100].replace("\n", " ")
            console.print(f"   ğŸ“„ æ¥æº: {top_result.metadata.filename}")
            console.print(f"   ğŸ’¯ å¾—åˆ†: {top_result.score:.4f}")
            console.print(f"   ğŸ“ é¢„è§ˆ: {preview}...")
        console.print()
    
    return vector_retriever, fulltext_retriever


async def step7_test_postprocessing(vector_retriever, fulltext_retriever):
    """æ­¥éª¤ 7: æµ‹è¯•åå¤„ç†"""
    print_section("ğŸ”„ æ­¥éª¤ 7: æµ‹è¯•åå¤„ç†", "-")
    
    query = "ä¸åŒè´·æ¬¾äº§å“çš„åˆ©ç‡å’Œé¦–ä»˜è¦æ±‚æ¯”è¾ƒ"
    
    console.print(f"æŸ¥è¯¢: [bold]{query}[/bold]\n")
    console.print("ä½¿ç”¨èåˆæ£€ç´¢ (RRF)...")
    fusion_retriever = QueryFusionRetriever(
        retrievers=[vector_retriever, fulltext_retriever],
        mode=FusionMode.RECIPROCAL_RANK,
        top_k=10
    )
    raw_results = fusion_retriever.retrieve(query)
    console.print(f"   åŸå§‹ç»“æœ: {len(raw_results)} ä¸ªå•å…ƒ")
    
    # åˆ›å»ºåå¤„ç†é“¾
    postprocessor = ChainPostprocessor([
        SimilarityFilter(threshold=0.6),
        Deduplicator(strategy="exact"),
        ContextAugmentor(window_size=1),
    ])
    
    console.print("\nåº”ç”¨åå¤„ç†é“¾:")
    console.print("   1. SimilarityFilter(threshold=0.6)")
    console.print("   2. Deduplicator(strategy='exact')")
    console.print("   3. ContextAugmentor(window_size=1)")
    
    processed_results = postprocessor.process(query, raw_results)
    console.print(f"\n   âœ… å¤„ç†åç»“æœ: {len(processed_results)} ä¸ªå•å…ƒ", style="bold green")
    
    # æ˜¾ç¤ºç»“æœ
    if processed_results:
        table = Table(title="åå¤„ç†ç»“æœ", show_header=True, header_style="bold magenta")
        table.add_column("#", style="cyan", width=4)
        table.add_column("å¾—åˆ†", style="green", width=10)
        table.add_column("æ¥æº", style="yellow", width=30)
        table.add_column("å†…å®¹é¢„è§ˆ", style="white", width=50)
        
        for i, unit in enumerate(processed_results[:5], 1):
            score = f"{unit.score:.4f}" if hasattr(unit, 'score') and unit.score else "N/A"
            source = unit.metadata.filename if hasattr(unit.metadata, 'filename') else "N/A"
            preview = unit.content[:47].replace("\n", " ") + "..."
            
            table.add_row(str(i), score, source, preview)
        
        console.print(table)
    
    return processed_results


async def main():
    """ä¸»æµç¨‹"""
    global _pipeline_start_time
    
    console.print("\n" + "=" * 70)
    console.print("  ğŸš€ æˆ¿è´·æŒ‡å— RAG Pipeline", style="bold cyan")
    console.print("=" * 70)
    console.print(f"\nğŸ“Œ é…ç½®: è¿è¡Œåˆ° Step {RUN_UNTIL_STEP}\n", style="yellow")
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_prerequisites():
        console.print("\nâŒ å‰ç½®æ¡ä»¶ä¸æ»¡è¶³ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜ã€‚", style="bold red")
        sys.exit(1)
    
    try:
        _pipeline_start_time = time.time()
        
        # æ‰§è¡Œæµç¨‹
        documents = await step1_read_documents()
        should_stop_after_step(1)
        
        units = await step2_split_documents(documents)
        should_stop_after_step(2)
        
        units = await step3_process_tables(units)
        should_stop_after_step(3)
        
        units = await step4_extract_metadata(units)
        should_stop_after_step(4)
        
        vector_indexer, fulltext_indexer = await step5_build_indices(units)
        should_stop_after_step(5)
        
        vector_retriever, fulltext_retriever = await step6_test_retrieval(vector_indexer, fulltext_indexer)
        should_stop_after_step(6)
        
        final_results = await step7_test_postprocessing(vector_retriever, fulltext_retriever)
        
        total_time = time.time() - _pipeline_start_time
        
        # æ€»ç»“
        print_section("ğŸ“Š Pipeline æ€»ç»“")
        console.print(f"âœ… E2E pipeline å®Œæˆï¼Œè€—æ—¶ {total_time:.2f}s", style="bold green")
        console.print(f"\nPipeline é˜¶æ®µ:")
        console.print(f"   1. âœ… æ–‡æ¡£è¯»å–: PDF â†’ Markdown")
        console.print(f"   2. âœ… æ–‡æ¡£åˆ‡åˆ†: åŸºäºæ ‡é¢˜ + é€’å½’åˆå¹¶")
        console.print(f"   3. âœ… è¡¨æ ¼å¤„ç†: è§£æ + æ‘˜è¦")
        console.print(f"   4. âœ… å…ƒæ•°æ®æå–: å…³é”®è¯")
        console.print(f"   5. âœ… ç´¢å¼•æ„å»º: å‘é‡ + å…¨æ–‡")
        console.print(f"   6. âœ… æ£€ç´¢æµ‹è¯•: èåˆæ£€ç´¢ç­–ç•¥")
        console.print(f"   7. âœ… åå¤„ç†: è¿‡æ»¤ + å»é‡ + å¢å¼º")
        
        console.print(f"\nğŸ’¡ å…³é”®æŒ‡æ ‡:")
        console.print(f"   - æ€»æ–‡æ¡£æ•°: {len(documents)}")
        console.print(f"   - æ€»å•å…ƒæ•°: {len(units)}")
        console.print(f"   - ç´¢å¼•æ–¹å¼: å‘é‡ + å…¨æ–‡åŒç´¢å¼•")
        console.print(f"   - æ£€ç´¢ç­–ç•¥: èåˆæ£€ç´¢ (RRF)")
        console.print(f"   - è¾“å‡ºç›®å½•: {OUTPUT_DIR}")
        
        console.print("\n" + "=" * 70)
        console.print("âœ… æµ‹è¯•æˆåŠŸå®Œæˆ!", style="bold green")
        console.print("=" * 70)
        
    except Exception as e:
        console.print(f"\nâŒ Pipeline å¤±è´¥: {e}", style="bold red")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
