#!/usr/bin/env python3
"""
Mortgage Guidelines RAG Pipeline

å¤„ç†æˆ¿è´·æŒ‡å—æ–‡æ¡£çš„å®Œæ•´ RAG æµç¨‹ï¼š
1. æ–‡æ¡£è¯»å– (PDF)
2. æ–‡æ¡£åˆ‡åˆ† (åŸºäº Markdown æ ‡é¢˜ + é€’å½’åˆå¹¶)
3. è¡¨æ ¼å¤„ç† (è§£æ + æ‘˜è¦)
4. å…ƒæ•°æ®æå– (å…³é”®è¯)
5. ç´¢å¼•æ„å»º (å‘é‡ + å…¨æ–‡)
6. æ£€ç´¢æµ‹è¯• (èåˆæ£€ç´¢)
7. åå¤„ç† (è¿‡æ»¤ + å»é‡ + ä¸Šä¸‹æ–‡å¢å¼º)

è¿è¡Œå‰å‡†å¤‡ï¼š
1. å¯åŠ¨ Meilisearch: ./meilisearch
2. è®¾ç½®ç¯å¢ƒå˜é‡ .env:
   BAILIAN_API_KEY=your-api-key
3. PDF æ–‡ä»¶åœ¨ rag/files/ ç›®å½•ä¸‹
4. GPU åŠ é€Ÿ (å¯é€‰):
   # å¸è½½ CPU ç‰ˆ PyTorch
   pip uninstall torch torchvision torchaudio
   
   # å®‰è£… CUDA ç‰ˆ PyTorch (ä»¥ CUDA 12.1 ä¸ºä¾‹)
   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
   
   # éªŒè¯ CUDA å¯ç”¨æ€§
   python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
"""

import os
import sys
import time
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from zag.readers.docling import DoclingReader
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.accelerator_options import AcceleratorDevice, AcceleratorOptions
from zag.splitters import MarkdownHeaderSplitter, TextSplitter, TableSplitter, RecursiveMergingSplitter
from zag.extractors import TableExtractor, KeywordExtractor
from zag.embedders import Embedder
from zag.storages.vector import ChromaVectorStore
from zag.indexers import VectorIndexer, FullTextIndexer
from zag.retrievers import VectorRetriever, FullTextRetriever, QueryFusionRetriever, FusionMode
from zag.postprocessors import (
    SimilarityFilter,
    Deduplicator,
    ContextAugmentor,
    ChainPostprocessor,
)
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.table import Table
from rich.panel import Panel

console = Console()

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®å‚æ•°
API_KEY = os.getenv("BAILIAN_API_KEY")  # ä¿ç•™ç”¨äºå¯èƒ½çš„è¿œç¨‹æ¨¡å‹
EMBEDDING_MODEL = "jina/jina-embeddings-v2-base-en:latest"  # Ollama æœ¬åœ°æ¨¡å‹
OLLAMA_BASE_URL = "http://localhost:11434"  # Ollama æœåŠ¡åœ°å€
LLM_MODEL = "qwen2.5:7b"  # ä½¿ç”¨ Ollama æœ¬åœ°åƒé—®æ¨¡å‹ï¼ˆè°ƒè¯•ç”¨ï¼‰
EMBEDDING_URI = f"ollama/{EMBEDDING_MODEL}"  # ä½¿ç”¨ Ollama embedder
LLM_URI = f"ollama/{LLM_MODEL}"  # ä½¿ç”¨ Ollama æœ¬åœ° LLMï¼ˆä¸éœ€è¦ API Keyï¼‰
MEILISEARCH_URL = "http://127.0.0.1:7700"
FILES_DIR = Path(__file__).parent / "files"
OUTPUT_ROOT = Path(__file__).parent / "output"  # æ ¹è¾“å‡ºç›®å½•
CHROMA_PERSIST_DIR = OUTPUT_ROOT / "chroma_db"  # å…±äº«çš„å‘é‡æ•°æ®åº“

# æµç¨‹æ§åˆ¶é…ç½®
RUN_UNTIL_STEP = 6  # è¿è¡Œåˆ°ç¬¬å‡ æ­¥å°±åœæ­¢ (1-7)ï¼Œè®¾ç½®ä¸º 7 è¡¨ç¤ºè¿è¡Œå®Œæ•´æµç¨‹

# åˆ›å»ºæ ¹è¾“å‡ºç›®å½•
OUTPUT_ROOT.mkdir(exist_ok=True)

# å…¨å±€å˜é‡ï¼šå½“å‰å¤„ç†æ–‡æ¡£çš„è¾“å‡ºç›®å½•ï¼ˆåœ¨ main() ä¸­è®¾ç½®ï¼‰
CURRENT_DOC_OUTPUT_DIR = None


def print_section(title: str, char: str = "="):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    console.print(f"\n{char * 70}")
    console.print(f"  {title}", style="bold cyan")
    console.print(f"{char * 70}\n")


# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨å¼€å§‹æ—¶é—´
_pipeline_start_time = None


def should_stop_after_step(step_num: int):
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨å½“å‰æ­¥éª¤ååœæ­¢"""
    if step_num >= RUN_UNTIL_STEP:
        total_time = time.time() - _pipeline_start_time
        console.print(f"\nâœ… æ‰§è¡Œåˆ° Step {step_num}ï¼Œå®Œæˆï¼(è€—æ—¶ {total_time:.2f}s)", style="bold green")
        console.print(f"\nğŸ’¡ æç¤º: ä¿®æ”¹ RUN_UNTIL_STEP é…ç½®å¯ä»¥è¿è¡Œæ›´å¤šæ­¥éª¤", style="yellow")
        sys.exit(0)


def check_prerequisites():
    """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
    print_section("ğŸ” æ£€æŸ¥å‰ç½®æ¡ä»¶")
    
    issues = []
    
    # æ£€æŸ¥ API Key (å¯é€‰ï¼Œä»…å½“ä½¿ç”¨è¿œç¨‹ LLM æ—¶éœ€è¦)
    if LLM_URI.startswith("bailian/"):
        if not API_KEY:
            issues.append("âŒ .env æ–‡ä»¶ä¸­æœªæ‰¾åˆ° BAILIAN_API_KEY (Bailian LLM éœ€è¦)")
        else:
            console.print(f"âœ… API Key å·²æ‰¾åˆ°: {API_KEY[:10]}...")
    else:
        console.print(f"ğŸ“¦ ä½¿ç”¨æœ¬åœ° LLM: {LLM_URI}ï¼ˆæ— éœ€ API Keyï¼‰")
    
    # æ£€æŸ¥ Ollama æœåŠ¡
    try:
        import httpx
        response = httpx.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5.0)
        if response.status_code == 200:
            console.print(f"âœ… Ollama æœåŠ¡è¿è¡Œä¸­: {OLLAMA_BASE_URL}")
        else:
            issues.append(f"âŒ Ollama æœåŠ¡å¼‚å¸¸: {response.status_code}")
    except Exception as e:
        issues.append(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama: {e}")
        console.print(f"   ğŸ’¡ æç¤º: è¯·ç¡®ä¿ Ollama å·²å¯åŠ¨ (ollama serve)")
    
    # æ£€æŸ¥ GPU/CUDA æ”¯æŒ
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        if cuda_available:
            gpu_name = torch.cuda.get_device_name(0)
            console.print(f"âœ… GPU å¯ç”¨: {gpu_name}")
            console.print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        else:
            console.print(f"âš ï¸  GPU ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPUï¼ˆæ€§èƒ½è¾ƒä½ï¼‰", style="yellow")
            console.print(f"   ğŸ’¡ æç¤º: å®‰è£… CUDA ç‰ˆ PyTorch ä»¥å¯ç”¨ GPU åŠ é€Ÿ")
            console.print(f"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
    except ImportError:
        issues.append("âŒ PyTorch æœªå®‰è£…")
    
    # æ£€æŸ¥ PDF æ–‡ä»¶
    pdf_files = list(FILES_DIR.glob("*.pdf"))
    if not pdf_files:
        issues.append(f"âŒ æœªæ‰¾åˆ° PDF æ–‡ä»¶: {FILES_DIR}")
    else:
        console.print(f"âœ… æ‰¾åˆ° {len(pdf_files)} ä¸ª PDF æ–‡ä»¶:")
        for pdf in pdf_files:
            size_mb = pdf.stat().st_size / (1024 * 1024)
            console.print(f"   - {pdf.name} ({size_mb:.1f} MB)")
    
    # æ£€æŸ¥ Meilisearch
    try:
        import meilisearch
        client = meilisearch.Client(MEILISEARCH_URL)
        health = client.health()
        if health.get("status") == "available":
            console.print(f"âœ… Meilisearch è¿è¡Œä¸­: {MEILISEARCH_URL}")
        else:
            issues.append("âŒ Meilisearch ä¸å¯ç”¨")
    except Exception as e:
        issues.append(f"âŒ æ— æ³•è¿æ¥åˆ° Meilisearch: {e}")
    
    if issues:
        console.print("\nâš ï¸  å‘ç°é—®é¢˜:", style="bold yellow")
        for issue in issues:
            console.print(f"  {issue}")
        return False
    
    console.print("\nâœ… æ‰€æœ‰å‰ç½®æ¡ä»¶æ»¡è¶³!", style="bold green")
    return True


async def step1_read_documents():
    """æ­¥éª¤ 1: è¯»å–æ‰€æœ‰ PDF æ–‡æ¡£ï¼ˆæ”¯æŒç¼“å­˜ + è´¨é‡éªŒè¯ï¼‰"""
    print_section("ğŸ“„ æ­¥éª¤ 1: è¯»å–æ–‡æ¡£", "-")
    
    pdf_files = sorted(FILES_DIR.glob("*.pdf"))
    console.print(f"å‡†å¤‡è¯»å– {len(pdf_files)} ä¸ª PDF æ–‡ä»¶...")
    
    # å¯¼å…¥è´¨é‡éªŒè¯å·¥å…·
    from validate_conversion import validate_cache_quality
    
    # é…ç½® DoclingReader
    pdf_options = PdfPipelineOptions()
    pdf_options.accelerator_options = AcceleratorOptions(
        num_threads=8,
        device=AcceleratorDevice.CUDA
    )
    
    reader = DoclingReader(pdf_pipeline_options=pdf_options)
    documents = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        console=console
    ) as progress:
        task = progress.add_task("[cyan]è¯»å–æ–‡æ¡£...", total=len(pdf_files))
        
        for pdf_path in pdf_files:
            # ä½¿ç”¨æ–‡æ¡£ä¸“å±çš„ raw/ å­ç›®å½•
            raw_dir = CURRENT_DOC_OUTPUT_DIR / "raw"
            raw_dir.mkdir(parents=True, exist_ok=True)
            markdown_path = raw_dir / f"{pdf_path.stem}.md"
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨ä¸”è´¨é‡åˆæ ¼
            use_cache = False
            if markdown_path.exists():
                console.print(f"\nğŸ” æ£€æŸ¥ç¼“å­˜è´¨é‡: {pdf_path.name}")
                is_valid = validate_cache_quality(pdf_path, markdown_path, threshold=90.0, verbose=False)
                
                if is_valid:
                    console.print(f"  âœ… ç¼“å­˜è´¨é‡åˆæ ¼ (>= 90åˆ†)ï¼Œä½¿ç”¨ç¼“å­˜")
                    use_cache = True
                else:
                    console.print(f"  âš ï¸  ç¼“å­˜è´¨é‡ä¸è¶³ (< 90åˆ†)ï¼Œé‡æ–°è§£æ PDF")
                    # åˆ é™¤ä½è´¨é‡ç¼“å­˜
                    markdown_path.unlink()
            
            if use_cache:
                # ä»ç¼“å­˜åŠ è½½
                with open(markdown_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æ„é€ ç®€åŒ–çš„å…ƒæ•°æ®
                from zag.schemas.base import DocumentMetadata, Page
                from zag.schemas.pdf import PDF
                
                metadata = DocumentMetadata(
                    source=str(pdf_path),
                    source_type="local",
                    file_type="pdf",
                    file_name=pdf_path.name,
                    file_size=pdf_path.stat().st_size,
                    file_extension=".pdf",
                    content_length=len(content),
                    reader_name="DoclingReader (cached)",
                    custom={
                        'cached': True,
                        'cache_file': str(markdown_path),
                        'quality_validated': True
                    }
                )
                
                # åˆ›å»ºç®€å•çš„ Page å¯¹è±¡
                pages = [Page(
                    page_number=1,
                    content={'texts': [], 'tables': [], 'pictures': []},
                    metadata={'cached': True}
                )]
                
                doc = PDF(
                    content=content,
                    metadata=metadata,
                    pages=pages
                )
                
                documents.append(doc)
                console.print(f"  âœ… å†…å®¹é•¿åº¦: {len(content):,} å­—ç¬¦ (ä»ç¼“å­˜)")
                
            else:
                # æ²¡æœ‰ç¼“å­˜æˆ–è´¨é‡ä¸åˆæ ¼ï¼Œä» PDF è¯»å–
                console.print(f"\nğŸ“„ è§£æ PDF: {pdf_path.name}")
                doc = reader.read(str(pdf_path))
                documents.append(doc)
                
                console.print(f"  âœ… å†…å®¹é•¿åº¦: {len(doc.content):,} å­—ç¬¦")
                console.print(f"  âœ… é¡µæ•°: {len(doc.pages)}")
                if doc.metadata.custom:
                    console.print(f"  âœ… æ–‡æœ¬é¡¹: {doc.metadata.custom.get('text_items_count', 0)}")
                    console.print(f"  âœ… è¡¨æ ¼é¡¹: {doc.metadata.custom.get('table_items_count', 0)}")
                
                # ä¿å­˜ Markdown å†…å®¹ä½œä¸ºç¼“å­˜
                with open(markdown_path, 'w', encoding='utf-8') as f:
                    f.write(doc.content)
                console.print(f"  âœ… Markdown å·²ä¿å­˜: {markdown_path.name}")
            
            progress.update(task, advance=1)
    
    console.print(f"\nâœ… å…±è¯»å– {len(documents)} ä¸ªæ–‡æ¡£", style="bold green")
    return documents


async def step2_split_documents(documents):
    """æ­¥éª¤ 2: åˆ‡åˆ†æ‰€æœ‰æ–‡æ¡£"""
    print_section("ğŸ”ª æ­¥éª¤ 2: åˆ‡åˆ†æ–‡æ¡£", "-")
    
    console.print("ä½¿ç”¨å®Œæ•´ Pipeline: MarkdownHeaderSplitter | TextSplitter | TableSplitter | RecursiveMergingSplitter")
    console.print("  - MarkdownHeaderSplitter: æŒ‰æ ‡é¢˜åˆ‡åˆ†")
    console.print("  - TextSplitter(1200 tokens): æ‰“æ–­è¶…å¤§å—")
    console.print("  - TableSplitter(1500 tokens): åˆ‡åˆ†è¶…å¤§è¡¨æ ¼")
    console.print("  - RecursiveMergingSplitter(800 tokens): åˆå¹¶å°å—åˆ°ç›®æ ‡å¤§å°\n")
    
    # æ„å»ºå®Œæ•´çš„åˆ‡åˆ† pipeline
    pipeline = (
        MarkdownHeaderSplitter()
        | TextSplitter(max_chunk_tokens=1200)
        | TableSplitter(max_chunk_tokens=1500)
        | RecursiveMergingSplitter(target_token_size=800)
    )
    
    all_units = []
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        console=console
    ) as progress:
        task = progress.add_task("[cyan]åˆ‡åˆ†æ–‡æ¡£...", total=len(documents))
        
        for doc in documents:
            units = doc.split(pipeline)
            all_units.extend(units)
            console.print(f"  {doc.metadata.file_name}: {len(units)} ä¸ªå•å…ƒ")
            progress.update(task, advance=1)
    
    # è®¡ç®— token ç»Ÿè®¡
    import tiktoken
    tokenizer = tiktoken.get_encoding("cl100k_base")
    token_counts = [len(tokenizer.encode(u.content)) for u in all_units]
    
    console.print(f"\nâœ… åˆ‡åˆ†å®Œæˆ:", style="bold green")
    console.print(f"   - æ€»å•å…ƒæ•°: {len(all_units)}")
    console.print(f"   - Token èŒƒå›´: {min(token_counts)}-{max(token_counts)} (å¹³å‡: {sum(token_counts)//len(token_counts)})")
    
    # Token åˆ†å¸ƒç»Ÿè®¡
    console.print(f"\nğŸ“Š Token åˆ†å¸ƒ:")
    ranges = [
        ("Tiny (<200)", 0, 200),
        ("Small (200-500)", 200, 500),
        ("Medium (500-1000)", 500, 1000),
        ("Large (1000-1500)", 1000, 1500),
        ("Oversized (>1500)", 1500, float('inf')),
    ]
    
    for label, low, high in ranges:
        count = sum(1 for t in token_counts if low <= t < high)
        if count > 0:
            pct = (count / len(token_counts)) * 100
            bar = "â–ˆ" * int(pct / 2)
            console.print(f"   {label:<20} {count:>4} ({pct:>5.1f}%) {bar}")
    
    # æ£€æŸ¥è¶…å¤§å—
    oversized = [(i, t) for i, t in enumerate(token_counts) if t > 1500]
    if oversized:
        console.print(f"\nâš ï¸  å‘ç° {len(oversized)} ä¸ªè¶…å¤§å•å…ƒ (>1500 tokens):", style="yellow")
        for idx, tokens in oversized[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            context = all_units[idx].metadata.context_path if all_units[idx].metadata else "N/A"
            console.print(f"   Unit {idx}: {tokens:,} tokens | {context[:50]}...")
        if len(oversized) > 5:
            console.print(f"   ... è¿˜æœ‰ {len(oversized) - 5} ä¸ª")
    
    # å¯¼å‡ºå¯è§†åŒ–æ–‡ä»¶
    from datetime import datetime
    console.print(f"\nğŸ’¾ å¯¼å‡ºå¯è§†åŒ–æ–‡ä»¶...")
    
    # ä½¿ç”¨æ–‡æ¡£ä¸“å±çš„ split/ å­ç›®å½•
    split_dir = CURRENT_DOC_OUTPUT_DIR / "split"
    split_dir.mkdir(parents=True, exist_ok=True)
    viz_dir = split_dir / "visualization"
    viz_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    # ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­è·å–æ–‡æ¡£å
    doc_name = documents[0].metadata.file_name.rsplit('.', 1)[0] if documents else "document"
    viz_file = viz_dir / f"{doc_name}_split_{timestamp}.md"
    
    with open(viz_file, 'w', encoding='utf-8') as f:
        # å†™å…¥å¤´éƒ¨
        f.write(f"# Mortgage Guidelines - Document Splitting Visualization\n\n")
        f.write(f"**Total Units**: {len(all_units)}\n\n")
        f.write(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Token Range**: {min(token_counts)}-{max(token_counts)} tokens\n\n")
        f.write(f"**Average**: {sum(token_counts)//len(token_counts)} tokens\n\n")
        f.write("---\n\n")
        
        # å†™å…¥æ¯ä¸ª unit
        for i, unit in enumerate(all_units):
            tokens = len(tokenizer.encode(unit.content))
            
            # è§†è§‰åˆ†éš”ç¬¦
            f.write(f"\n\n")
            f.write(f"{'ğŸ”·' * 50}\n\n")
            
            # Unit å¤´éƒ¨
            f.write(f"## ğŸ“¦ Unit {i} | {tokens} tokens\n\n")
            
            # å…ƒæ•°æ®ä¿¡æ¯
            if hasattr(unit, 'metadata') and unit.metadata:
                if hasattr(unit.metadata, 'context_path') and unit.metadata.context_path:
                    f.write(f"**Context**: {unit.metadata.context_path}\n\n")
            
            # å†…å®¹é¢„è§ˆ
            preview = unit.content.strip()[:100].replace('\n', ' ')
            f.write(f"**Preview**: {preview}...\n\n")
            
            # Token å¤§å°æŒ‡ç¤º
            if tokens > 1500:
                f.write(f"âš ï¸ **OVERSIZED** ({tokens} tokens)\n\n")
            elif tokens >= 1000:
                f.write(f"ğŸ“Š **LARGE** ({tokens} tokens)\n\n")
            
            f.write(f"---\n\n")
            
            # å®é™…å†…å®¹
            f.write(unit.content)
            f.write("\n\n")
    
    console.print(f"   âœ… å·²ä¿å­˜åˆ°: {viz_file.name}", style="green")
    console.print(f"   ğŸ“ ä½ç½®: {viz_dir}")
    
    return all_units


async def step3_process_tables(units):
    """æ­¥éª¤ 3: å¤„ç†è¡¨æ ¼ (è§£æ + æ‘˜è¦)"""
    print_section("ğŸ“Š æ­¥éª¤ 3: å¤„ç†è¡¨æ ¼", "-")
    
    # æ£€æŸ¥ç¼“å­˜
    tables_dir = CURRENT_DOC_OUTPUT_DIR / "tables"
    tables_dir.mkdir(parents=True, exist_ok=True)
    units_json_path = tables_dir / "units_after_table_processing.json"
    
    if units_json_path.exists():
        console.print(f"ğŸ” å‘ç°ç¼“å­˜æ–‡ä»¶: {units_json_path.name}")
        console.print(f"   è·³è¿‡è¡¨æ ¼å¤„ç†ï¼Œç›´æ¥åŠ è½½ç¼“å­˜...")
        
        import json
        from zag.schemas.unit import TextUnit, TableUnit
        from zag.schemas.base import UnitType
        
        with open(units_json_path, 'r', encoding='utf-8') as f:
            units_data = json.load(f)
        
        # é‡å»º Unit å¯¹è±¡ï¼ˆæ ¹æ® unit_type é€‰æ‹©ç±»ï¼‰
        units = []
        for data in units_data:
            unit_type = data.get('unit_type', 'TEXT')
            if unit_type == 'TABLE' or unit_type == UnitType.TABLE.value:
                units.append(TableUnit(**data))
            else:
                units.append(TextUnit(**data))
        
        console.print(f"âœ… å·²ä»ç¼“å­˜åŠ è½½ {len(units)} ä¸ªå•å…ƒ", style="bold green")
        units_with_embedding = sum(1 for u in units if hasattr(u, 'embedding_content') and u.embedding_content)
        console.print(f"   åŒ…å« embedding_content çš„å•å…ƒ: {units_with_embedding}")
        
        return units
    
    # æ²¡æœ‰ç¼“å­˜ï¼Œæ‰§è¡Œå¤„ç†
    console.print(f"ä½¿ç”¨ LLM æå–è¡¨æ ¼ä¿¡æ¯: {LLM_URI}")
    extractor = TableExtractor(llm_uri=LLM_URI, api_key=API_KEY)
    
    # æ‰¹é‡æå–
    results = await extractor.aextract(units)
    
    # æ›´æ–° embedding_content
    for unit, metadata in zip(units, results):
        if metadata.get("embedding_content"):
            unit.embedding_content = metadata["embedding_content"]
    
    console.print(f"âœ… å·²å¤„ç† {len(units)} ä¸ªå•å…ƒ", style="bold green")
    
    # ä¿å­˜å¤„ç†åçš„ units åˆ° JSONï¼ˆä½¿ç”¨æ–‡æ¡£ä¸“å±çš„ tables/ å­ç›®å½•ï¼‰
    import json
    units_data = [unit.model_dump(mode='json') for unit in units]
    
    with open(units_json_path, 'w', encoding='utf-8') as f:
        json.dump(units_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"ğŸ’¾ å·²ä¿å­˜å¤„ç†åçš„ units: {units_json_path.name}")
    console.print(f"   æ€»å•å…ƒæ•°: {len(units)}")
    
    # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    units_with_embedding = sum(1 for u in units if hasattr(u, 'embedding_content') and u.embedding_content)
    console.print(f"   åŒ…å« embedding_content çš„å•å…ƒ: {units_with_embedding}")
    
    return units


async def step4_extract_metadata(units):
    """æ­¥éª¤ 4: æå–å…ƒæ•°æ® (å…³é”®è¯)"""
    print_section("ğŸ·ï¸  æ­¥éª¤ 4: æå–å…ƒæ•°æ®", "-")
    
    # æ£€æŸ¥ç¼“å­˜
    metadata_dir = CURRENT_DOC_OUTPUT_DIR / "metadata"
    metadata_dir.mkdir(parents=True, exist_ok=True)
    units_json_path = metadata_dir / "units_after_keyword_extraction.json"
    
    if units_json_path.exists():
        console.print(f"ğŸ” å‘ç°ç¼“å­˜æ–‡ä»¶: {units_json_path.name}")
        console.print(f"   è·³è¿‡å…³é”®è¯æå–ï¼Œç›´æ¥åŠ è½½ç¼“å­˜...")
        
        import json
        from zag.schemas.unit import TextUnit, TableUnit
        from zag.schemas.base import UnitType
        
        with open(units_json_path, 'r', encoding='utf-8') as f:
            units_data = json.load(f)
        
        # é‡å»º Unit å¯¹è±¡ï¼ˆæ ¹æ® unit_type é€‰æ‹©ç±»ï¼‰
        units = []
        for data in units_data:
            unit_type = data.get('unit_type', 'TEXT')
            if unit_type == 'TABLE' or unit_type == UnitType.TABLE.value:
                units.append(TableUnit(**data))
            else:
                units.append(TextUnit(**data))
        
        console.print(f"âœ… å·²ä»ç¼“å­˜åŠ è½½ {len(units)} ä¸ªå•å…ƒ", style="bold green")
        units_with_keywords = sum(1 for u in units if u.metadata.custom.get('excerpt_keywords'))
        console.print(f"   åŒ…å«å…³é”®è¯çš„å•å…ƒ: {units_with_keywords}")
        
        return units
    
    # æ²¡æœ‰ç¼“å­˜ï¼Œæ‰§è¡Œæå–
    console.print(f"ä¸ºæ‰€æœ‰å•å…ƒæå–å…³é”®è¯: {LLM_URI}")
    extractor = KeywordExtractor(
        llm_uri=LLM_URI,
        api_key=API_KEY,
        num_keywords=5
    )
    
    # æ‰¹é‡æå–
    results = await extractor.aextract(units)
    
    # æ›´æ–°å…ƒæ•°æ®
    for unit, metadata in zip(units, results):
        unit.metadata.custom.update(metadata)
    
    console.print(f"âœ… å·²ä¸º {len(units)} ä¸ªå•å…ƒæå–å…³é”®è¯", style="bold green")
    console.print("\nç¤ºä¾‹å…³é”®è¯ (å‰ 3 ä¸ªå•å…ƒ):")
    for i, unit in enumerate(units[:3], 1):
        keywords = unit.metadata.custom.get("excerpt_keywords", [])
        console.print(f"   {i}. {keywords}")
    
    # ä¿å­˜æå–å…³é”®è¯åçš„ units åˆ° JSONï¼ˆä½¿ç”¨æ–‡æ¡£ä¸“å±çš„ metadata/ å­ç›®å½•ï¼‰
    import json
    units_data = [unit.model_dump(mode='json') for unit in units]
    
    with open(units_json_path, 'w', encoding='utf-8') as f:
        json.dump(units_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"\nğŸ’¾ å·²ä¿å­˜å¤„ç†åçš„ units: {units_json_path.name}")
    console.print(f"   æ€»å•å…ƒæ•°: {len(units)}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    units_with_keywords = sum(1 for u in units if u.metadata.custom.get('excerpt_keywords'))
    console.print(f"   åŒ…å«å…³é”®è¯çš„å•å…ƒ: {units_with_keywords}")
    
    return units


async def step5_build_indices(units):
    """æ­¥éª¤ 5: æ„å»ºç´¢å¼• (å‘é‡ + å…¨æ–‡)"""
    print_section("ğŸ“š æ­¥éª¤ 5: æ„å»ºç´¢å¼•", "-")
    
    # ä¿å­˜ units åˆ° JSON ä»¥ä¾›æ£€æŸ¥ï¼ˆä½¿ç”¨æ–‡æ¡£ä¸“å±çš„ metadata/ å­ç›®å½•ï¼‰
    import json
    metadata_dir = CURRENT_DOC_OUTPUT_DIR / "metadata"
    metadata_dir.mkdir(parents=True, exist_ok=True)
    units_json_path = metadata_dir / "units_data.json"
    units_data = [unit.model_dump(mode='json') for unit in units]
    
    with open(units_json_path, 'w', encoding='utf-8') as f:
        json.dump(units_data, f, ensure_ascii=False, indent=2)
    
    console.print(f"Units æ•°æ®å·²ä¿å­˜åˆ°: {units_json_path}")
    console.print(f"æ€»å•å…ƒæ•°: {len(units)}\n")
    
    # 5.1 å‘é‡ç´¢å¼•
    console.print("æ„å»ºå‘é‡ç´¢å¼•...")
    console.print(f"   ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹: {EMBEDDING_MODEL}")
    embedder = Embedder(EMBEDDING_URI)
    
    vector_store = ChromaVectorStore.local(
        path=str(CHROMA_PERSIST_DIR),
        collection_name="mortgage_guidelines",
        embedder=embedder
    )
    console.print(f"   æŒä¹…åŒ–ç›®å½•: {CHROMA_PERSIST_DIR}")
    
    vector_indexer = VectorIndexer(vector_store=vector_store)
    # æ¸…ç©ºç°æœ‰æ•°æ®
    await vector_indexer.aclear()
    await vector_indexer.aadd(units)
    console.print(f"   âœ… å‘é‡ç´¢å¼•å·²æ„å»º: {vector_indexer.count()} ä¸ªå•å…ƒ", style="bold green")
    
    # 5.2 å…¨æ–‡ç´¢å¼•
    console.print("\næ„å»ºå…¨æ–‡ç´¢å¼•...")
    fulltext_indexer = FullTextIndexer(
        url=MEILISEARCH_URL,
        index_name="mortgage_guidelines",
        primary_key="unit_id"
    )
    
    # æ¸…ç©ºç°æœ‰æ•°æ®
    fulltext_indexer.clear()
    fulltext_indexer.configure_settings(
        searchable_attributes=["content", "context_path"],
        filterable_attributes=["unit_type", "source_doc_id"],
        sortable_attributes=["created_at"],
    )
    fulltext_indexer.add(units)
    console.print(f"   âœ… å…¨æ–‡ç´¢å¼•å·²æ„å»º: {fulltext_indexer.count()} ä¸ªå•å…ƒ", style="bold green")
    
    return vector_indexer, fulltext_indexer


async def step6_test_retrieval(retriever_type: str = "fusion"):
    """
    æ­¥éª¤ 6: æµ‹è¯•æ£€ç´¢åŠŸèƒ½ï¼ˆå¯ç‹¬ç«‹è¿è¡Œï¼‰
    
    Args:
        retriever_type: æ£€ç´¢å™¨ç±»å‹ï¼Œå¯é€‰ "vector", "fulltext", "fusion"
    """
    print_section(f"ğŸ” æ­¥éª¤ 6: æµ‹è¯•æ£€ç´¢ ({retriever_type.upper()})", "-")
    
    # è‡ªå·±æ„å»º vector_store å’Œ indexerï¼ˆä»æŒä¹…åŒ–æ•°æ®åŠ è½½ï¼‰
    console.print("åˆå§‹åŒ–æ£€ç´¢å™¨...")
    console.print(f"   å‘é‡æ•°æ®åº“: {CHROMA_PERSIST_DIR}")
    console.print(f"   å…¨æ–‡ç´¢å¼•: {MEILISEARCH_URL}")
    console.print(f"   æ£€ç´¢ç±»å‹: {retriever_type.upper()}\n")
    
    embedder = Embedder(EMBEDDING_URI)
    vector_store = ChromaVectorStore.local(
        path=str(CHROMA_PERSIST_DIR),
        collection_name="mortgage_guidelines",
        embedder=embedder
    )
    vector_indexer = VectorIndexer(vector_store=vector_store)
    console.print(f"âœ… å‘é‡ç´¢å¼•å·²åŠ è½½: {vector_indexer.count()} ä¸ªå•å…ƒ")
    
    fulltext_indexer = FullTextIndexer(
        url=MEILISEARCH_URL,
        index_name="mortgage_guidelines",
        primary_key="unit_id"
    )
    console.print(f"âœ… å…¨æ–‡ç´¢å¼•å·²åŠ è½½: {fulltext_indexer.count()} ä¸ªå•å…ƒ\n")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "FHA è´·æ¬¾çš„é¦–ä»˜è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
        "VA è´·æ¬¾çš„èµ„æ ¼æ¡ä»¶æœ‰å“ªäº›ï¼Ÿ",
        "Fannie Mae çš„ LTV è¦æ±‚",
        "USDA è´·æ¬¾é€‚ç”¨çš„åœ°åŒº",
        "Freddie Mac çš„åˆ©ç‡æ”¿ç­–",
    ]
    
    # åˆ›å»ºæ£€ç´¢å™¨
    vector_retriever = VectorRetriever(vector_store=vector_indexer.vector_store, top_k=5)
    fulltext_retriever = FullTextRetriever(url=MEILISEARCH_URL, index_name="mortgage_guidelines", top_k=5)
    
    # æ ¹æ®ç±»å‹é€‰æ‹©æ£€ç´¢å™¨
    if retriever_type == "vector":
        retriever = vector_retriever
    elif retriever_type == "fulltext":
        retriever = fulltext_retriever
    elif retriever_type == "fusion":
        # åˆ›å»ºèåˆæ£€ç´¢å™¨
        retriever = QueryFusionRetriever(
            retrievers=[vector_retriever, fulltext_retriever],
            mode=FusionMode.RECIPROCAL_RANK,
            top_k=3
        )
    else:
        raise ValueError(f"Unknown retriever_type: {retriever_type}")
    
    console.print("æµ‹è¯•æŸ¥è¯¢ç¤ºä¾‹:\n")
    for i, query in enumerate(test_queries, 1):
        console.print(f"[bold cyan]{i}. æŸ¥è¯¢:[/bold cyan] {query}")
        
        start = time.time()
        results = retriever.retrieve(query)
        elapsed = time.time() - start
        
        console.print(f"   âœ… æ‰¾åˆ° {len(results)} ä¸ªç»“æœ ({elapsed*1000:.0f}ms)")
        
        if results:
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªç»“æœ
            top_result = results[0]
            preview = top_result.content[:100].replace("\n", " ")
            
            # è·å–æ¥æºä¿¡æ¯ï¼šä¼˜å…ˆä½¿ç”¨ context_pathï¼Œå…¶æ¬¡ä½¿ç”¨ sourceæ£€ç´¢æ¥æº
            source_info = "N/A"
            if top_result.metadata and top_result.metadata.context_path:
                source_info = top_result.metadata.context_path.split('/')[0]  # å–ç¬¬ä¸€çº§è·¯å¾„
            if top_result.source:
                source_info = f"{source_info} ({top_result.source.value})"
            
            console.print(f"   ğŸ“„ æ¥æº: {source_info}")
            console.print(f"   ğŸ’¯ å¾—åˆ†: {top_result.score:.4f}")
            console.print(f"   ğŸ“ é¢„è§ˆ: {preview}...")
        console.print()
    
    return vector_retriever, fulltext_retriever


async def step7_test_postprocessing():
    """æ­¥éª¤ 7: æµ‹è¯•åå¤„ç†ï¼ˆå¯ç‹¬ç«‹è¿è¡Œï¼‰"""
    print_section("ğŸ”„ æ­¥éª¤ 7: æµ‹è¯•åå¤„ç†", "-")
    
    # è‡ªå·±æ„å»º retrieverï¼ˆä»æŒä¹…åŒ–æ•°æ®åŠ è½½ï¼‰
    console.print("åˆå§‹åŒ–æ£€ç´¢å™¨...")
    embedder = Embedder(EMBEDDING_URI)
    vector_store = ChromaVectorStore.local(
        path=str(CHROMA_PERSIST_DIR),
        collection_name="mortgage_guidelines",
        embedder=embedder
    )
    vector_retriever = VectorRetriever(vector_store=vector_store, top_k=5)
    fulltext_retriever = FullTextRetriever(url=MEILISEARCH_URL, index_name="mortgage_guidelines", top_k=5)
    console.print("âœ… æ£€ç´¢å™¨å·²åˆå§‹åŒ–\n")
    
    query = "ä¸åŒè´·æ¬¾äº§å“çš„åˆ©ç‡å’Œé¦–ä»˜è¦æ±‚æ¯”è¾ƒ"
    
    console.print(f"æŸ¥è¯¢: [bold]{query}[/bold]\n")
    console.print("ä½¿ç”¨èåˆæ£€ç´¢ (RRF)...")
    fusion_retriever = QueryFusionRetriever(
        retrievers=[vector_retriever, fulltext_retriever],
        mode=FusionMode.RECIPROCAL_RANK,
        top_k=10
    )
    raw_results = fusion_retriever.retrieve(query)
    console.print(f"   åŸå§‹ç»“æœ: {len(raw_results)} ä¸ªå•å…ƒ")
    
    # åˆ›å»ºåå¤„ç†é“¾
    postprocessor = ChainPostprocessor([
        SimilarityFilter(threshold=0.6),
        Deduplicator(strategy="exact"),
        ContextAugmentor(window_size=1),
    ])
    
    console.print("\nåº”ç”¨åå¤„ç†é“¾:")
    console.print("   1. SimilarityFilter(threshold=0.6)")
    console.print("   2. Deduplicator(strategy='exact')")
    console.print("   3. ContextAugmentor(window_size=1)")
    
    processed_results = postprocessor.process(query, raw_results)
    console.print(f"\n   âœ… å¤„ç†åç»“æœ: {len(processed_results)} ä¸ªå•å…ƒ", style="bold green")
    
    # æ˜¾ç¤ºç»“æœ
    if processed_results:
        table = Table(title="åå¤„ç†ç»“æœ", show_header=True, header_style="bold magenta")
        table.add_column("#", style="cyan", width=4)
        table.add_column("å¾—åˆ†", style="green", width=10)
        table.add_column("æ¥æº", style="yellow", width=30)
        table.add_column("å†…å®¹é¢„è§ˆ", style="white", width=50)
        
        for i, unit in enumerate(processed_results[:5], 1):
            score = f"{unit.score:.4f}" if hasattr(unit, 'score') and unit.score else "N/A"
            
            # è·å–æ¥æºä¿¡æ¯
            source = "N/A"
            if unit.metadata and unit.metadata.context_path:
                source = unit.metadata.context_path.split('/')[0]  # å–ç¬¬ä¸€çº§è·¯å¾„
            if hasattr(unit, 'source') and unit.source:
                source = f"{source} ({unit.source.value})"
            
            preview = unit.content[:47].replace("\n", " ") + "..."
            
            table.add_row(str(i), score, source, preview)
        
        console.print(table)
    
    return processed_results


async def main():
    """ä¸»æµç¨‹"""
    global _pipeline_start_time, CURRENT_DOC_OUTPUT_DIR
    
    console.print("\n" + "=" * 70)
    console.print("  ğŸš€ æˆ¿è´·æŒ‡å— RAG Pipeline", style="bold cyan")
    console.print("=" * 70)
    console.print(f"\nğŸ“Œ é…ç½®: è¿è¡Œåˆ° Step {RUN_UNTIL_STEP}\n", style="yellow")
    
    # æ£€æŸ¥å‰ç½®æ¡ä»¶
    if not check_prerequisites():
        console.print("\nâŒ å‰ç½®æ¡ä»¶ä¸æ»¡è¶³ï¼Œè¯·ä¿®å¤ä¸Šè¿°é—®é¢˜ã€‚", style="bold red")
        sys.exit(1)
    
    # æ£€æµ‹ PDF æ–‡ä»¶ï¼Œåˆ›å»ºæ–‡æ¡£ä¸“å±è¾“å‡ºç›®å½•
    pdf_files = sorted(FILES_DIR.glob("*.pdf"))
    if not pdf_files:
        console.print("\nâŒ æœªæ‰¾åˆ° PDF æ–‡ä»¶", style="bold red")
        sys.exit(1)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ª PDF çš„æ–‡ä»¶åä½œä¸ºè¾“å‡ºç›®å½•åï¼ˆå»é™¤æ‰©å±•åï¼‰
    doc_name = pdf_files[0].stem
    CURRENT_DOC_OUTPUT_DIR = OUTPUT_ROOT / doc_name
    
    # åˆ›å»ºæ–‡æ¡£ä¸“å±å­ç›®å½•ç»“æ„
    subdirs = ["raw", "split", "tables", "metadata", "indices"]
    for subdir in subdirs:
        (CURRENT_DOC_OUTPUT_DIR / subdir).mkdir(parents=True, exist_ok=True)
    
    console.print(f"\nğŸ“ æ–‡æ¡£è¾“å‡ºç›®å½•: {CURRENT_DOC_OUTPUT_DIR}")
    console.print(f"   å­ç›®å½•: {', '.join(subdirs)}\n", style="dim")
    
    try:
        _pipeline_start_time = time.time()
        
        # æ‰§è¡Œæµç¨‹
        documents = await step1_read_documents()
        should_stop_after_step(1)
        
        units = await step2_split_documents(documents)
        should_stop_after_step(2)
        
        units = await step3_process_tables(units)
        should_stop_after_step(3)
        
        units = await step4_extract_metadata(units)
        should_stop_after_step(4)
        
        vector_indexer, fulltext_indexer = await step5_build_indices(units)
        should_stop_after_step(5)
        
        # åˆ†åˆ«æµ‹è¯•ä¸‰ç§æ£€ç´¢æ–¹å¼
        console.print("\n" + "=" * 70)
        console.print("  ğŸ¯ å¼€å§‹æ£€ç´¢æµ‹è¯•", style="bold cyan")
        console.print("=" * 70)
        
        await step6_test_retrieval(retriever_type="vector")
        console.print("\n" + "-" * 70 + "\n")
        
        await step6_test_retrieval(retriever_type="fulltext")
        console.print("\n" + "-" * 70 + "\n")
        
        await step6_test_retrieval(retriever_type="fusion")
        should_stop_after_step(6)
                
        final_results = await step7_test_postprocessing()
        
        total_time = time.time() - _pipeline_start_time
        
        # æ€»ç»“
        print_section("ğŸ“Š Pipeline æ€»ç»“")
        console.print(f"âœ… E2E pipeline å®Œæˆï¼Œè€—æ—¶ {total_time:.2f}s", style="bold green")
        console.print(f"\nPipeline é˜¶æ®µ:")
        console.print(f"   1. âœ… æ–‡æ¡£è¯»å–: PDF â†’ Markdown")
        console.print(f"   2. âœ… æ–‡æ¡£åˆ‡åˆ†: åŸºäºæ ‡é¢˜ + é€’å½’åˆå¹¶")
        console.print(f"   3. âœ… è¡¨æ ¼å¤„ç†: è§£æ + æ‘˜è¦")
        console.print(f"   4. âœ… å…ƒæ•°æ®æå–: å…³é”®è¯")
        console.print(f"   5. âœ… ç´¢å¼•æ„å»º: å‘é‡ + å…¨æ–‡")
        console.print(f"   6. âœ… æ£€ç´¢æµ‹è¯•: èåˆæ£€ç´¢ç­–ç•¥")
        console.print(f"   7. âœ… åå¤„ç†: è¿‡æ»¤ + å»é‡ + å¢å¼º")
        
        console.print(f"\nğŸ’¡ å…³é”®æŒ‡æ ‡:")
        console.print(f"   - æ€»æ–‡æ¡£æ•°: {len(documents)}")
        console.print(f"   - æ€»å•å…ƒæ•°: {len(units)}")
        console.print(f"   - ç´¢å¼•æ–¹å¼: å‘é‡ + å…¨æ–‡åŒç´¢å¼•")
        console.print(f"   - æ£€ç´¢ç­–ç•¥: èåˆæ£€ç´¢ (RRF)")
        console.print(f"   - è¾“å‡ºç›®å½•: {CURRENT_DOC_OUTPUT_DIR}")
        
        console.print("\n" + "=" * 70)
        console.print("âœ… æµ‹è¯•æˆåŠŸå®Œæˆ!", style="bold green")
        console.print("=" * 70)
        
    except Exception as e:
        console.print(f"\nâŒ Pipeline å¤±è´¥: {e}", style="bold red")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
